# 逻辑回归

## 1. 假设函数 (Hypothesis Function)

逻辑回归使用 Sigmoid 函数作为假设函数，将线性输出映射到 (0, 1) 区间，表示概率。

$$
f_{\mathbf{w},b}(\mathbf{x}) = \sigma(\mathbf{w} \cdot \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w} \cdot \mathbf{x} + b)}}
$$

- $\mathbf{w}$: 权重向量 (weight vector)
- $b$: 偏置项 (bias term)
- $\mathbf{x}$: 输入特征向量 (input feature vector)
- $f_{\mathbf{w},b}(\mathbf{x})$: 样本属于正类（y=1）的预测概率

---

## 2. 损失函数与代价函数 (Loss & Cost Function)

### 单个样本的损失函数

使用交叉熵损失函数 (Cross-Entropy Loss)：

$$
L(f_{\mathbf{w},b}(\mathbf{x}), y) = 
\begin{cases}
-\log(f_{\mathbf{w},b}(\mathbf{x})) & \text{if } y = 1 \\
-\log(1 - f_{\mathbf{w},b}(\mathbf{x})) & \text{if } y = 0
\end{cases}
$$

合并为一个简洁的公式：

$$
L(f_{\mathbf{w},b}(\mathbf{x}), y) = -\left[ y \log(f_{\mathbf{w},b}(\mathbf{x})) + (1 - y) \log(1 - f_{\mathbf{w},b}(\mathbf{x})) \right]
$$

### 整个数据集的代价函数

对所有 $m$ 个样本的损失求平均：

$$
J(\mathbf{w}, b) = \frac{1}{m} \sum_{i=1}^{m} L(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(f_{\mathbf{w},b}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - f_{\mathbf{w},b}(\mathbf{x}^{(i)})) \right]
$$

---

## 3. 梯度下降更新规则 (Gradient Descent Update Rule)

目标是最小化代价函数 $J(\mathbf{w}, b)$，通过迭代更新参数：

**同时更新 (Simultaneous update) 所有参数：**
$$
\begin{aligned}
w_j &= w_j - \alpha \frac{\partial}{\partial w_j} J(\mathbf{w}, b) \quad \text{for } j = 1, ..., n \\
b &= b - \alpha \frac{\partial}{\partial b} J(\mathbf{w}, b)
\end{aligned}
$$

- $\alpha$: 学习率 (learning rate)

**最终得到的梯度结果：**
$$
\begin{aligned}
\frac{\partial}{\partial w_j} J(\mathbf{w}, b) &= \frac{1}{m} \sum_{i=1}^{m} \left[ (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \right] x_j^{(i)} \\
\frac{\partial}{\partial b} J(\mathbf{w}, b) &= \frac{1}{m} \sum_{i=1}^{m} \left[ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)} \right]
\end{aligned}
$$

推导 $\frac{\partial}{\partial w_j} J(\mathbf{w}, b)$ 的过程是理解的关键。我们使用链式法则 (Chain Rule)。

---

首先，定义两个中间变量以简化求导：
1. 线性组合 $z$: 
   $$z = \mathbf{w} \cdot \mathbf{x} + b$$
2. Sigmoid 输出 $f$:
   $$f = \sigma(z) = \frac{1}{1 + e^{-z}}$$

我们的目标是求 $\frac{\partial L}{\partial w_j}$（单个样本），而 $\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum \frac{\partial L^{(i)}}{\partial w_j}$。

#### 步骤 1: 计算损失 $L$ 对预测值 $f$ 的导数

$$
\begin{aligned}
\frac{\partial L}{\partial f} &= \frac{\partial}{\partial f} \left[ -\left( y \log(f) + (1 - y) \log(1 - f) \right) \right] \\
&= -\left[ y \cdot \frac{1}{f} + (1 - y) \cdot \frac{1}{1 - f} \cdot (-1) \right] \\
&= -\left[ \frac{y}{f} - \frac{1 - y}{1 - f} \right]
\end{aligned}
$$

通分合并分子：
$$
\begin{aligned}
\frac{\partial L}{\partial f} &= -\left[ \frac{y(1-f) - (1-y)f}{f(1-f)} \right] \\
&= -\left[ \frac{y - yf - f + yf}{f(1-f)} \right] \\
&= -\left[ \frac{y - f}{f(1-f)} \right] = \frac{f - y}{f(1 - f)}
\end{aligned}
$$

#### 步骤 2: 计算预测值 $f$ 对线性输出 $z$ 的导数

Sigmoid 函数的导数有其特殊形式：
$$
\begin{aligned}
\frac{\partial f}{\partial z} &= \frac{\partial}{\partial z} \left( \frac{1}{1 + e^{-z}} \right) \\
&= \frac{-1}{(1 + e^{-z})^2} \cdot (-e^{-z}) \\
&= \frac{e^{-z}}{(1 + e^{-z})^2} \\
&= \frac{1}{1 + e^{-z}} \cdot \frac{e^{-z}}{1 + e^{-z}} \\
&= f \cdot (1 - f)
\end{aligned}
$$

**重要结论：** $\frac{\partial f}{\partial z} = f(1 - f)$

#### 步骤 3: 计算线性输出 $z$ 对权重 $w_j$ 的导数

$$
\frac{\partial z}{\partial w_j} = \frac{\partial}{\partial w_j} (\mathbf{w} \cdot \mathbf{x} + b) = x_j
$$

#### 步骤 4: 应用链式法则合并

根据链式法则：
$$
\frac{\partial L}{\partial w_j} = \frac{\partial L}{\partial f} \cdot \frac{\partial f}{\partial z} \cdot \frac{\partial z}{\partial w_j}
$$

将前三步的结果代入：
$$
\begin{aligned}
\frac{\partial L}{\partial w_j} &= \left[ \frac{f - y}{f(1 - f)} \right] \cdot \left[ f(1 - f) \right] \cdot x_j \\
&= (f - y) \cdot x_j
\end{aligned}
$$

**$f(1-f)$** 项被完美地约掉了，得到了一个非常简洁的结果。
 
#### 步骤 5: 从单个样本推广到整个代价函数

对于单个样本 $i$，其损失对 $w_j$ 的偏导为 $(f^{(i)} - y^{(i)}) x_j^{(i)}$。
整个代价函数 $J$ 是平均损失，因此其对 $w_j$ 的偏导也是所有样本偏导的平均值：

$$
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} \frac{\partial L^{(i)}}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}
$$

同理，因为 $\frac{\partial z}{\partial b} = 1$，所以：
$$
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})
$$

---

## 总结与直观理解

- **最终形式**：逻辑回归的权重更新公式为 $\frac{1}{m} \sum (预测值 - 真实值) \times 特征值$，与线性回归在形式上完全一致，但其内核（假设函数 $f$）完全不同。
- **推导关键**：Sigmoid 函数的导数 $\frac{df}{dz} = f(1-f)$ 在链式求导中恰好与 $\frac{\partial L}{\partial f}$ 的分母相抵消，简化了最终结果。
- **直观理解**：更新幅度取决于：
    1.  **预测误差** $(f - y)$：误差越大，更新步长越大。
    2.  **特征值 $x_j$**：该特征值越大，其对误差的“贡献”或“责任”就越大，因此对应权重的调整也越大。
    3.  **学习率 $\alpha$**：控制整体的更新步长。
    4.  **平均项 $\frac{1}{m}$**：确保更新幅度不受训练集大小 $m$ 的影响，使用批量梯度下降。
