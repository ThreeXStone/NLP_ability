# 监督学习 vs 无监督学习

机器学习主要可以分为以下几大类：

## 1. 监督学习 (Supervised Learning)

**核心思想**：模型从**带有标签**的训练数据中学习，从而对新的、未见过的数据做出预测。

-   **主要任务**：
    -   **分类**：预测离散的类别标签。
        -   *例如*：判断一封邮件是「垃圾邮件」还是「正常邮件」；识别图片中的动物是「猫」还是「狗」。
    -   **回归**：预测连续的数值。
        -   *例如*：根据房屋面积和地段预测房价；根据历史数据预测明天的气温。

-   **常用算法**：
    -   `线性回归`、`逻辑回归`
    -   `支持向量机`、`决策树`
    -   `随机森林`、`神经网络`

## 2. 无监督学习 (Unsupervised Learning)

**核心思想**：模型从**无标签**的数据中自行发现内在结构和模式。

-   **主要任务**：
    -   **聚类**：将数据分成不同的组（簇），组内样本相似度高，组间相似度低。
        -   *例如*：根据用户的购买行为对用户进行分组；对文章进行主题分类。
    -   **降维**：在尽可能保留关键信息的前提下，减少数据的变量数量。
        -   *例如*：可视化高维数据（如将数据降至2D/3D便于绘图）；去除冗余特征。

-   **常用算法**：
    -   `K-Means` 聚类
    -   `主成分分析` 用于降维
    -   `自编码器`

# 线性回归与梯度下降

## 1. 模型
线性回归模型通过一个线性函数来建立特征 `x` 与目标值 `y` 之间的关系：

$f_{w,b}(x) = wx + b$

## 2. 成本函数 (Cost Function)

成本函数 
$$ J(w, b) = \frac{1}{2m} $$
（也称为损失函数）用于衡量模型预测值与真实值之间的平均误差。对于线性回归，我们使用**均方误差**：

$$ J(w, b) = \frac{1}{2m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$

- \( m \)：训练样本的数量
- \( (x^{(i)}, y^{(i)}) \)：第 \( i \) 个训练样本
- 乘以 \( \frac{1}{2} \) 是为了后续计算梯度时简化数学表达式

## 3. 梯度下降算法 (Gradient Descent Algorithm)

梯度下降是一种通过迭代更新参数来最小化成本函数的优化算法。

### 算法步骤：
重复以下过程直至收敛：

`\[
\begin{aligned}
w &:= w - \alpha \cdot \frac{\partial}{\partial w} J(w, b) \\
b &:= b - \alpha \cdot \frac{\partial}{\partial b} J(w, b)
\end{aligned}
\]`

### 关键概念：
- `\( \alpha \)`: **学习率**，控制每次更新参数的步长
- `\( \frac{\partial}{\partial w} J(w, b) \)`: 成本函数对权重 `\( w \)` 的偏导数
- `\( \frac{\partial}{\partial b} J(w, b) \)`: 成本函数对偏置 `\( b \)` 的偏导数

## 4. 导数计算 (Computing the Derivatives)

对于线性回归的均方误差成本函数，偏导数的具体计算公式为：

`\[
\begin{aligned}
\frac{\partial}{\partial w} J(w, b) &= \frac{1}{m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)}) \cdot x^{(i)} \\
\frac{\partial}{\partial b} J(w, b) &= \frac{1}{m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})
\end{aligned}
\]`
