<img width="944" height="183" alt="image" src="https://github.com/user-attachments/assets/7a599998-91d5-46a9-b195-54f81862657b" /># 线性回归与梯度下降

## 1. 模型
线性回归模型通过一个线性函数来建立特征 `x` 与目标值 `y` 之间的关系：

$f_{w,b}(x) = wx + b$

## 2. 成本函数 (Cost Function)

成本函数（也称为损失函数）用于衡量模型预测值与真实值之间的平均误差。对于线性回归，我们使用**均方误差**：

$ J(w, b) = \frac{1}{2m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2 $

$$ J(w, b) = \frac{1}{2m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$

- \( m \)：训练样本的数量
- \( (x^{(i)}, y^{(i)}) \)：第 \( i \) 个训练样本
- 乘以 \( \frac{1}{2} \) 是为了后续计算梯度时简化数学表达式

## 3. 梯度下降算法 (Gradient Descent Algorithm)

梯度下降是一种通过迭代更新参数来最小化成本函数的优化算法。

### 算法步骤：
重复以下过程直至收敛：

`\[
\begin{aligned}
w &:= w - \alpha \cdot \frac{\partial}{\partial w} J(w, b) \\
b &:= b - \alpha \cdot \frac{\partial}{\partial b} J(w, b)
\end{aligned}
\]`

### 关键概念：
- `\( \alpha \)`: **学习率**，控制每次更新参数的步长
- `\( \frac{\partial}{\partial w} J(w, b) \)`: 成本函数对权重 `\( w \)` 的偏导数
- `\( \frac{\partial}{\partial b} J(w, b) \)`: 成本函数对偏置 `\( b \)` 的偏导数

## 4. 导数计算 (Computing the Derivatives)

对于线性回归的均方误差成本函数，偏导数的具体计算公式为：

`\[
\begin{aligned}
\frac{\partial}{\partial w} J(w, b) &= \frac{1}{m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)}) \cdot x^{(i)} \\
\frac{\partial}{\partial b} J(w, b) &= \frac{1}{m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})
\end{aligned}
\]`
