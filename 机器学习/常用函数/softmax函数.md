Softmax函数将一个任意实数的K维向量"压缩"为另一个K维实向量，**使得每个元素的范围在(0,1)之间，并且所有元素的和为1**。数学表达式为：
$$
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} \quad \text{其中} \quad i = 1, 2, \ldots, K
$$  

Softmax函数的主要作用是将神经网络的原始输出（logits）转换为**概率分布**。这使得我们可以：

- 解释每个类别的预测概率

- 比较不同类别的相对可能性

- 做出基于概率的决策  

在实际计算中，**为了避免数值溢出**（由于指数函数增长非常快），通常使用以下**稳定版本**：
$$
\sigma(\mathbf{z})_i = \frac{e^{z_i - \max(\mathbf{z})}}{\sum_{j=1}^K e^{z_j - \max(\mathbf{z})}}
$$  

Softmax函数通常与交叉熵损失函数结合使用，形成深度学习中最常用的损失函数之一:
$$
L = -\sum_{i=1}^K y_i \log(\sigma(\mathbf{z})_i)
$$  
其中，$y_i$是真实标签（one-hot编码），$\sigma(\mathbf{z})_i$是Softmax输出的预测概率。这种组合的**梯度计算**非常简单，有利于**反向传播算法**的实施。
